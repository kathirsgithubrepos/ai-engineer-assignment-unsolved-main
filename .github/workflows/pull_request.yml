name: Pull Request Validation

on:
  pull_request:
    branches: [ "main" ]

permissions:
  contents: read
  pull-requests: write

jobs:
  evaluate:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout PR branch
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies (PR)
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # ----- Run solution on PR branch -----
      - name: Run solution (PR)
        run: |
          python scripts/solution.py \
            --employees_path data/employees.csv \
            --connections_path data/connections.csv \
            --output_path pr_submission.csv

      - name: Evaluate PR accuracy
        id: eval_pr
        run: |
          python dependencies/evaluate.py pr_submission.csv data/ground_truth_managers.csv
          PR=$(cat accuracy.txt | tr -d '%')
          echo "pr_score=$PR" >> "$GITHUB_OUTPUT"

      # ----- Run solution on main branch in a separate directory -----
      - name: Checkout main into ./main_ref
        uses: actions/checkout@v4
        with:
          ref: main
          path: main_ref

      - name: Install dependencies (main)
        run: |
          pip install -r main_ref/requirements.txt

      - name: Run solution (main)
        working-directory: main_ref
        run: |
          python scripts/solution.py \
            --employees_path data/employees.csv \
            --connections_path data/connections.csv \
            --output_path main_submission.csv

      - name: Evaluate main accuracy
        id: eval_main
        working-directory: main_ref
        run: |
          python dependencies/evaluate.py main_submission.csv data/ground_truth_managers.csv
          MAIN=$(cat accuracy.txt | tr -d '%')
          echo "main_score=$MAIN" >> "$GITHUB_OUTPUT"

      # ----- Comment results on the PR -----
      - name: Comment PR results
        uses: actions/github-script@v7
        with:
          script: |
            const prScore = Number("${{ steps.eval_pr.outputs.pr_score }}");
            const mainScore = Number("${{ steps.eval_main.outputs.main_score }}");
            const verdict = prScore > mainScore
              ? "✔️ Improved"
              : (prScore === mainScore ? "➖ No change" : "❌ Regression");
            const body = `
            ### Automated Evaluation

            | Metric   | Main | PR |
            |---------|-----:|---:|
            | Accuracy | ${mainScore}% | ${prScore}% |

            **Verdict:** ${verdict}
            `;
                        github.rest.issues.createComment({
                          owner: context.repo.owner,
                          repo: context.repo.repo,
                          issue_number: context.issue.number,
                          body
                        });

                  # ----- Fail the job if PR is worse than main -----
                  - name: Fail if PR accuracy worsens
                    if: ${{ fromJSON(steps.eval_pr.outputs.pr_score) < fromJSON(steps.eval_main.outputs.main_score) }}
                    run: exit 1
